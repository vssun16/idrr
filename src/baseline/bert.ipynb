{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IDRR_data import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path as path\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (Trainer, TrainingArguments, AutoModelForSequenceClassification, DataCollatorWithPadding, AutoTokenizer)\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®å¯è§çš„GPUè®¾å¤‡\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4,5'\n",
    "\n",
    "# èŽ·å–å½“å‰æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•å’Œæ ¹ç›®å½•\n",
    "SRC_DIR = path(os.getcwd())\n",
    "ROOT_DIR = SRC_DIR.parent\n",
    "\n",
    "# === dataset ===\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, label_list, tokenizer) -> None:\n",
    "        self.df:pd.DataFrame = df\n",
    "        label_num = len(label_list)\n",
    "        self.ys = np.eye(label_num, label_num)[self.df['label11id']]\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        model_inputs = self.tokenizer(\n",
    "            row['arg1'], row['arg2'],\n",
    "            add_special_tokens=True, \n",
    "            padding=True,\n",
    "            truncation='longest_first', \n",
    "            max_length=512,\n",
    "        )\n",
    "        model_inputs['labels'] = self.ys[index]\n",
    "        return model_inputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "# === metric ===\n",
    "class ComputeMetrics:\n",
    "    def __init__(self, label_list:list) -> None:\n",
    "        self.label_list = label_list\n",
    "        self.num_labels = len(label_list)\n",
    "        self.metric_names = ['Macro-F1', 'Acc']\n",
    "    \n",
    "    def __call__(self, eval_pred):\n",
    "        \"\"\"\n",
    "        n = label categories\n",
    "        eval_pred: (pred, labels)\n",
    "        # pred: np.array [datasize, ]\n",
    "        pred: np.array [datasize, n]\n",
    "        labels: np.array [datasize, n]\n",
    "        X[p][q]=True, sample p belongs to label q (False otherwise)\n",
    "        \"\"\"\n",
    "        pred, labels = eval_pred\n",
    "        pred: np.ndarray\n",
    "        labels: np.ndarray\n",
    "        \n",
    "        # TODO \n",
    "        pred = pred[..., :len(self.label_list)]\n",
    "        labels = labels[..., :len(self.label_list)]\n",
    "        \n",
    "        # pred = pred!=0\n",
    "        max_indices = np.argmax(pred, axis=1)\n",
    "        bpred = np.zeros_like(pred, dtype=int)\n",
    "        bpred[np.arange(pred.shape[0]), max_indices] = 1\n",
    "        pred = bpred\n",
    "        assert ( pred.sum(axis=1)<=1 ).sum() == pred.shape[0]\n",
    "        labels = labels!=0\n",
    "        \n",
    "        res = {\n",
    "            'macro-f1': f1_score(labels, pred, average='macro', zero_division=0),\n",
    "            'acc': np.sum(pred*labels)/len(pred),\n",
    "        } \n",
    "        return res\n",
    "\n",
    "# === callback ===\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def __init__(\n",
    "        self, \n",
    "        log_filepath=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if log_filepath:\n",
    "            self.log_filepath = log_filepath\n",
    "        else:\n",
    "            self.log_filepath = ROOT_DIR / 'output_dir' / 'log.jsonl'\n",
    "    \n",
    "    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        with open(self.log_filepath, 'a', encoding='utf8')as f:\n",
    "            f.write(str(kwargs['logs'])+'\\n')\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics:Dict[str, float], **kwargs):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "input_ids [0, 1121, 41, 1700, 4, 753, 1551, 9, 22, 133, 256, 22471, 26636, 2379, 113, 23, 1568, 18, 17899, 5132, 6697, 38494, 8632, 1538, 37885, 4624, 5, 11897, 11, 7247, 219, 412, 60, 1063, 25145, 359, 4455, 238, 5, 774, 9, 13230, 757, 2552, 6, 702, 30, 1636, 230, 44156, 1250, 6, 21, 26506, 9702, 7, 15990, 4936, 1073, 2, 2, 13123, 4, 4936, 1073, 1974, 1448, 9488, 118, 2]\n",
      "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "labels [1. 0. 0. 0.]\n",
      "input_ids [0, 8827, 6006, 12569, 11120, 6934, 1723, 603, 4, 1130, 63, 3472, 7, 158, 3205, 31, 707, 3205, 10, 458, 2, 2, 133, 92, 731, 40, 28, 21467, 1927, 4, 379, 2]\n",
      "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "labels [0. 0. 1. 0.]\n",
      "input_ids [0, 133, 92, 731, 40, 28, 21467, 1927, 4, 379, 2, 2, 250, 638, 1248, 2282, 75, 57, 278, 2]\n",
      "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "labels [1. 0. 0. 0.]\n",
      "input_ids [0, 104, 5675, 5627, 9428, 472, 33, 7, 28, 3904, 114, 51, 236, 7, 173, 10, 319, 6, 142, 49, 30730, 8, 2437, 2868, 32, 1804, 2, 2, 133, 6168, 139, 661, 14370, 329, 11002, 10168, 34, 551, 10, 543, 516, 59, 5, 936, 35, 91, 20772, 8, 35803, 352, 36705, 1872, 39185, 8708, 4391, 8, 473, 103, 7909, 6, 98, 37, 630, 75, 33, 7, 310, 5, 276, 17295, 2013, 8, 25055, 4192, 366, 81, 8, 81, 456, 2]\n",
      "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "labels [0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# === data ===\n",
    "dfs = IDRRDataFrames(\n",
    "    data_name='pdtb2',\n",
    "    data_level='top',\n",
    "    data_relation='Implicit',\n",
    "    data_path='/data/sunwh/data/IDRR/used/pdtb2.p1.csv',\n",
    ")\n",
    "label_list = dfs.label_list\n",
    "\n",
    "print(len(label_list))\n",
    "\n",
    "checkpoint = '/data/sunwh/model/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# åŠ è½½è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†\n",
    "train_dataset = CustomDataset(dfs.train_df, label_list, tokenizer)\n",
    "dev_dataset = CustomDataset(dfs.dev_df, label_list, tokenizer)\n",
    "test_dataset = CustomDataset(dfs.test_df, label_list, tokenizer)\n",
    "for _, it in zip(range(4), train_dataset):\n",
    "    for key, value in it.items():\n",
    "        print(key, value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /data/sunwh/model/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/sunwh/miniconda/envs/main/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_61485/3244752691.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/data/sunwh/miniconda/envs/main/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3950' max='3950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3950/3950 18:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro-f1</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.343600</td>\n",
       "      <td>0.380456</td>\n",
       "      <td>0.502219</td>\n",
       "      <td>0.625528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.252800</td>\n",
       "      <td>0.379013</td>\n",
       "      <td>0.564206</td>\n",
       "      <td>0.659341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.415397</td>\n",
       "      <td>0.570217</td>\n",
       "      <td>0.666948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.501373</td>\n",
       "      <td>0.562684</td>\n",
       "      <td>0.652578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.570460</td>\n",
       "      <td>0.546849</td>\n",
       "      <td>0.654269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.609507</td>\n",
       "      <td>0.554605</td>\n",
       "      <td>0.647506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.668324</td>\n",
       "      <td>0.571225</td>\n",
       "      <td>0.656805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/sunwh/miniconda/envs/main/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/sunwh/miniconda/envs/main/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/sunwh/miniconda/envs/main/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/sunwh/miniconda/envs/main/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/sunwh/miniconda/envs/main/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/sunwh/miniconda/envs/main/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/sunwh/miniconda/envs/main/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/sunwh/miniconda/envs/main/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> train_result:\n",
      "  TrainOutput(global_step=3950, training_loss=0.18411906539688483, metrics={'train_runtime': 1104.8651, 'train_samples_per_second': 114.331, 'train_steps_per_second': 3.575, 'total_flos': 6528774075203328.0, 'train_loss': 0.18411906539688483, 'epoch': 10.0})\n",
      "> test_result:\n",
      "  {'eval_loss': 0.6380148446288656, 'eval_macro-f1': 0.6014715506085475, 'eval_acc': 0.6739961759082218, 'eval_runtime': 5.3902, 'eval_samples_per_second': 194.057, 'eval_steps_per_second': 12.244, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(label_list))\n",
    "\n",
    "# === args ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=ROOT_DIR/'output_dir',\n",
    "    overwrite_output_dir=True,\n",
    "    run_name='',\n",
    "    \n",
    "    # strategies of evaluation, logging, save\n",
    "    evaluation_strategy = \"steps\", \n",
    "    eval_steps = 500,\n",
    "    logging_strategy = 'steps',\n",
    "    logging_steps = 10,\n",
    "    save_strategy = 'steps',\n",
    "    save_steps = 500,\n",
    "    # max_steps=2,\n",
    "    \n",
    "    # optimizer and lr_scheduler\n",
    "    optim = 'adamw_torch',\n",
    "    # optim = 'sgd',\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = 'linear',\n",
    "    warmup_ratio = 0.05,\n",
    "    \n",
    "    # epochs and batches \n",
    "    num_train_epochs = 10, \n",
    "    # max_steps = args.max_steps,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    \n",
    "    # train consumption\n",
    "    eval_accumulation_steps=10,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "# === train ===\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=ComputeMetrics(dfs.label_list),\n",
    "    callbacks=[CustomCallback()],\n",
    ")\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒå’Œè¯„ä¼°\n",
    "train_result = trainer.train()\n",
    "test_result = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(f'> train_result:\\n  {train_result}')\n",
    "print(f'> test_result:\\n  {test_result}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
